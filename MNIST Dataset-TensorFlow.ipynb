{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.keras.callbacks' from 'C:\\\\Users\\\\mubashar.nazar\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\keras\\\\callbacks\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "tf.keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data reading\n",
    "data = pd.read_csv(\"./train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:]\n",
    "Y = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mubashar.nazar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X = X.as_matrix().reshape([X.shape[0], 28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot encoding\n",
    "Y = pd.get_dummies(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0, n_C0], name=\"x\")\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, n_y], name=\"y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "\n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,1,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters, outputs):\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    print (X.shape)\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    print (Z1.shape)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding=\"SAME\")\n",
    "    print (P1.shape)\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    print (Z2.shape)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "    print (P2.shape)\n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    print (P2.shape)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, outputs, activation_fn=None)\n",
    "    print (Z3.shape)\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01,\n",
    "          num_epochs = 1000, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters, len(Y_train.columns))\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    \n",
    "#     regularizer = tf.nn.l2_loss(parameters[\"W1\"]) + tf.nn.l2_loss(parameters[\"W2\"])\n",
    "#     cost = tf.reduce_mean(cost + 0.001 * regularizer, name=\"cost\")\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"optimizer\").minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "              \n",
    "            _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:X_train, Y:Y_train})\n",
    "#             saver.save(sess, 'model_iter', global_step=epoch)\n",
    "#             print (temp_cost)\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, temp_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(temp_cost)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "#         predict_op = Y_train.columns[predict_op]\n",
    "        tf.add_to_collection(\"predict_op\", predict_op)\n",
    "#         tf.add_to_collection(\"Z3\", Z3)\n",
    "        \n",
    "        best = sess.run([predict_op],feed_dict={X:X_train})\n",
    "#         print(best)\n",
    "        \n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#         print(accuracy)\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "#         print (predict_op.eval({X:X_train}))\n",
    "#         print (predict_op.eval({X:X_test}))\n",
    "#         print (Z3.eval({X:X_test}))\n",
    "#         last_chkp = saver.save(sess, 'graph')\n",
    "        saver.save(sess, save_path=\"graph\")\n",
    "    \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n",
      "(?, 28, 28, 8)\n",
      "(?, 4, 4, 8)\n",
      "(?, 4, 4, 16)\n",
      "(?, 1, 1, 16)\n",
      "(?, 16)\n",
      "(?, 10)\n",
      "Cost after epoch 0: 226.943893\n",
      "Cost after epoch 5: 41.436584\n",
      "Cost after epoch 10: 13.325080\n",
      "Cost after epoch 15: 6.645814\n",
      "Cost after epoch 20: 4.278826\n",
      "Cost after epoch 25: 3.050441\n",
      "Cost after epoch 30: 2.679555\n",
      "Cost after epoch 35: 2.461791\n",
      "Cost after epoch 40: 2.366124\n",
      "Cost after epoch 45: 2.303601\n",
      "Cost after epoch 50: 2.257425\n",
      "Cost after epoch 55: 2.222624\n",
      "Cost after epoch 60: 2.188486\n",
      "Cost after epoch 65: 2.154454\n",
      "Cost after epoch 70: 2.119947\n",
      "Cost after epoch 75: 2.087211\n",
      "Cost after epoch 80: 2.053699\n",
      "Cost after epoch 85: 2.022174\n",
      "Cost after epoch 90: 1.992800\n",
      "Cost after epoch 95: 1.966807\n",
      "Cost after epoch 100: 1.941885\n",
      "Cost after epoch 105: 1.916685\n",
      "Cost after epoch 110: 1.891867\n",
      "Cost after epoch 115: 1.868026\n",
      "Cost after epoch 120: 1.845291\n",
      "Cost after epoch 125: 1.823341\n",
      "Cost after epoch 130: 1.801984\n",
      "Cost after epoch 135: 1.780913\n",
      "Cost after epoch 140: 1.759647\n",
      "Cost after epoch 145: 1.738025\n",
      "Cost after epoch 150: 1.716941\n",
      "Cost after epoch 155: 1.696778\n",
      "Cost after epoch 160: 1.677300\n",
      "Cost after epoch 165: 1.658273\n",
      "Cost after epoch 170: 1.639827\n",
      "Cost after epoch 175: 1.622070\n",
      "Cost after epoch 180: 1.604148\n",
      "Cost after epoch 185: 1.586229\n",
      "Cost after epoch 190: 1.568365\n",
      "Cost after epoch 195: 1.550831\n",
      "Cost after epoch 200: 1.533695\n",
      "Cost after epoch 205: 1.517177\n",
      "Cost after epoch 210: 1.501259\n",
      "Cost after epoch 215: 1.486801\n",
      "Cost after epoch 220: 1.473142\n",
      "Cost after epoch 225: 1.460175\n",
      "Cost after epoch 230: 1.447955\n",
      "Cost after epoch 235: 1.436404\n",
      "Cost after epoch 240: 1.425393\n",
      "Cost after epoch 245: 1.414704\n",
      "Cost after epoch 250: 1.404219\n",
      "Cost after epoch 255: 1.393879\n",
      "Cost after epoch 260: 1.383766\n",
      "Cost after epoch 265: 1.373840\n",
      "Cost after epoch 270: 1.364041\n",
      "Cost after epoch 275: 1.354206\n",
      "Cost after epoch 280: 1.344411\n",
      "Cost after epoch 285: 1.334941\n",
      "Cost after epoch 290: 1.325854\n",
      "Cost after epoch 295: 1.316966\n",
      "Cost after epoch 300: 1.308212\n",
      "Cost after epoch 305: 1.299647\n",
      "Cost after epoch 310: 1.291141\n",
      "Cost after epoch 315: 1.282614\n",
      "Cost after epoch 320: 1.274222\n",
      "Cost after epoch 325: 1.265959\n",
      "Cost after epoch 330: 1.257967\n",
      "Cost after epoch 335: 1.250177\n",
      "Cost after epoch 340: 1.242459\n",
      "Cost after epoch 345: 1.234883\n",
      "Cost after epoch 350: 1.227258\n",
      "Cost after epoch 355: 1.218666\n",
      "Cost after epoch 360: 1.210266\n",
      "Cost after epoch 365: 1.200826\n",
      "Cost after epoch 370: 1.191976\n",
      "Cost after epoch 375: 1.183373\n",
      "Cost after epoch 380: 1.174836\n",
      "Cost after epoch 385: 1.166123\n",
      "Cost after epoch 390: 1.157585\n",
      "Cost after epoch 395: 1.149191\n",
      "Cost after epoch 400: 1.140893\n",
      "Cost after epoch 405: 1.132589\n",
      "Cost after epoch 410: 1.124228\n",
      "Cost after epoch 415: 1.115872\n",
      "Cost after epoch 420: 1.107572\n",
      "Cost after epoch 425: 1.099327\n",
      "Cost after epoch 430: 1.091080\n",
      "Cost after epoch 435: 1.082746\n",
      "Cost after epoch 440: 1.074526\n",
      "Cost after epoch 445: 1.066527\n",
      "Cost after epoch 450: 1.058622\n",
      "Cost after epoch 455: 1.050759\n",
      "Cost after epoch 460: 1.043093\n",
      "Cost after epoch 465: 1.035604\n",
      "Cost after epoch 470: 1.028262\n",
      "Cost after epoch 475: 1.021058\n",
      "Cost after epoch 480: 1.013997\n",
      "Cost after epoch 485: 1.006961\n",
      "Cost after epoch 490: 0.999255\n",
      "Cost after epoch 495: 0.991091\n",
      "Cost after epoch 500: 0.983920\n",
      "Cost after epoch 505: 0.977127\n",
      "Cost after epoch 510: 0.970200\n",
      "Cost after epoch 515: 0.963590\n",
      "Cost after epoch 520: 0.957142\n",
      "Cost after epoch 525: 0.950809\n",
      "Cost after epoch 530: 0.944472\n",
      "Cost after epoch 535: 0.938086\n",
      "Cost after epoch 540: 0.931579\n",
      "Cost after epoch 545: 0.924810\n",
      "Cost after epoch 550: 0.917536\n",
      "Cost after epoch 555: 0.909403\n",
      "Cost after epoch 560: 0.900075\n",
      "Cost after epoch 565: 0.889574\n",
      "Cost after epoch 570: 0.878681\n",
      "Cost after epoch 575: 0.867986\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
